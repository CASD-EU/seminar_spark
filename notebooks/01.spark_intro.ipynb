{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 01",
   "id": "584c85886203e884"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:12:25.215349Z",
     "start_time": "2025-04-17T10:12:21.858604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, lower, col, count\n",
    "\n",
    "# Initialize a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "5542b3be6adc7b54",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:12:30.779418Z",
     "start_time": "2025-04-17T10:12:29.287873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data path\n",
    "file_path = \"../data/le_petit_prince.txt\"\n",
    "# Read text file into DataFrame\n",
    "# Each line becomes a row in the DataFrame\n",
    "df = spark.read.text(file_path)"
   ],
   "id": "51319dcfe7ecd97",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:12:37.419435Z",
     "start_time": "2025-04-17T10:12:33.650254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process the text and count words\n",
    "word_counts = df \\\n",
    "    .select(explode(split(lower(col(\"value\")), \"\\\\s+\")).alias(\"word\")) \\\n",
    "    .filter(col(\"word\") != \"\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show top 20 most frequent words\n",
    "word_counts.show(50)  \n"
   ],
   "id": "9fa4c2408c2d0ca8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|     le|  454|\n",
      "|      -|  434|\n",
      "|     de|  428|\n",
      "|     je|  316|\n",
      "|     et|  283|\n",
      "|     il|  260|\n",
      "|    les|  249|\n",
      "|     un|  230|\n",
      "|     la|  219|\n",
      "|  petit|  193|\n",
      "|      à|  178|\n",
      "|     ne|  169|\n",
      "|    que|  154|\n",
      "|    pas|  148|\n",
      "|     tu|  136|\n",
      "|    des|  131|\n",
      "|  c'est|  126|\n",
      "|    dit|  125|\n",
      "|      ?|  125|\n",
      "|      !|  123|\n",
      "|   mais|  123|\n",
      "|    une|  123|\n",
      "|     me|  103|\n",
      "|   pour|   98|\n",
      "|    qui|   97|\n",
      "|     ce|   94|\n",
      "|   bien|   92|\n",
      "|     se|   85|\n",
      "|      :|   84|\n",
      "|  comme|   80|\n",
      "|     en|   79|\n",
      "|    est|   77|\n",
      "| prince|   74|\n",
      "|     du|   71|\n",
      "|     si|   67|\n",
      "|     ça|   66|\n",
      "|    sur|   66|\n",
      "|   dans|   63|\n",
      "|   j'ai|   62|\n",
      "|    mon|   61|\n",
      "|   elle|   58|\n",
      "|   plus|   53|\n",
      "|     au|   51|\n",
      "|prince.|   50|\n",
      "|   tout|   49|\n",
      "|   très|   49|\n",
      "|    lui|   47|\n",
      "|    par|   47|\n",
      "|    son|   46|\n",
      "|     on|   46|\n",
      "+-------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:13:01.452437Z",
     "start_time": "2025-04-17T10:12:59.906559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save results if needed\n",
    "word_counts.write.csv(\"word_count_results\", header=True)"
   ],
   "id": "55b22e4955207f9d",
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/PLIU/Documents/git/seminar_spark/notebooks/word_count_results already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAnalysisException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Save results if needed\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mword_counts\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mword_count_results\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\seminar_spark\\sp_venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001B[39m, in \u001B[36mDataFrameWriter.csv\u001B[39m\u001B[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[39m\n\u001B[32m   1845\u001B[39m \u001B[38;5;28mself\u001B[39m.mode(mode)\n\u001B[32m   1846\u001B[39m \u001B[38;5;28mself\u001B[39m._set_opts(\n\u001B[32m   1847\u001B[39m     compression=compression,\n\u001B[32m   1848\u001B[39m     sep=sep,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1862\u001B[39m     lineSep=lineSep,\n\u001B[32m   1863\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1864\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\seminar_spark\\sp_venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\seminar_spark\\sp_venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    181\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    183\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    184\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAnalysisException\u001B[39m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/PLIU/Documents/git/seminar_spark/notebooks/word_count_results already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ],
   "id": "4af5b229da4ab006"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
